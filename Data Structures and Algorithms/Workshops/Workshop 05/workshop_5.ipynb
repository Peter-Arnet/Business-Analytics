{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 5: Accessing data on the web\n",
    "\n",
    "_BS1819 Data Structures and Algorithms, September 2018_\n",
    "\n",
    "_Imperial College Business School_\n",
    "\n",
    "\n",
    "---\n",
    "In this workshop we'll practice web scraping and dealing with data from APIs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "This workshop is meant to be open-ended, and you don't need to submit anything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trump tweets, JSON edition\n",
    "\n",
    "Earlier in the course, we looked at some of Donald Trump's tweets that had been conveniently packaged into a CSV file that we could open as a spreadsheet. The dataset we used ended in summer 2016. What if we wanted to analyse more recent tweets? We could do this by registering for access to [Twitter's API](https://dev.twitter.com/rest/public) and downloading the data directly using one of the [libraries](https://dev.twitter.com/resources/twitter-libraries) that Python users have developed for accessing the API. \n",
    "\n",
    "We won't register for the API today. Instead, we'll use the data maintained by Github user [bpb27](https://github.com/bpb27/trump_tweet_data_archive). \n",
    "\n",
    "The data that we'll use has been cleaned and condensed, but resembles the output from the Twitter API. Instead of CSV, the API gives data in JSON format. We've included the file `condensed_2016.json` downloaded from Github in the zip file.\n",
    "\n",
    "We can read the JSON file to Python as we would a text file, or open it in Notepad or another text editor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"source\": \"Twitter for iPhone\", \"id_str\": \"815271067749060609\", \"text\": \"RT @realDonaldTrump: Happy Birthday @DonaldJTrumpJr!\\nhttps://t.co/uRxyCD3hBz\", \"created_at\": \"Sat Dec 31 18:59:04 +0000 2016\", \"retweet_count\": 9529, \"in_reply_to_user_id_str\": null, \"favorite_count\": 0, \"is_retweet\": true}, {\"source\": \"Twitter for iPhone\", \"id_str\": \"815270850916208644\", \"text\": \"Happy Birthday @DonaldJTrumpJr!\\nhttps://t.co/uRxyCD3hBz\", \"created_at\": \"Sat Dec 31 18:58:12 +0000 2016\", \"retweet_count\": 9\n"
     ]
    }
   ],
   "source": [
    "json_file_name = 'condensed_2016.json'\n",
    "with open(json_file_name, encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Print first characters of resulting string \n",
    "print(text[0:500]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having read the file, we could parse through the string looking for different aspects of each tweet. But it's much more convenient to use a library that directly exploits the structure of JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json_file_name = 'condensed_2016.json'\n",
    "with open(json_file_name, encoding='utf8') as f:\n",
    "        tweet_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a Python list of dictionaries containing the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'created_at': 'Sat Dec 31 18:59:04 +0000 2016',\n",
       "  'favorite_count': 0,\n",
       "  'id_str': '815271067749060609',\n",
       "  'in_reply_to_user_id_str': None,\n",
       "  'is_retweet': True,\n",
       "  'retweet_count': 9529,\n",
       "  'source': 'Twitter for iPhone',\n",
       "  'text': 'RT @realDonaldTrump: Happy Birthday @DonaldJTrumpJr!\\nhttps://t.co/uRxyCD3hBz'},\n",
       " {'created_at': 'Sat Dec 31 18:58:12 +0000 2016',\n",
       "  'favorite_count': 55601,\n",
       "  'id_str': '815270850916208644',\n",
       "  'in_reply_to_user_id_str': None,\n",
       "  'is_retweet': False,\n",
       "  'retweet_count': 9529,\n",
       "  'source': 'Twitter for iPhone',\n",
       "  'text': 'Happy Birthday @DonaldJTrumpJr!\\nhttps://t.co/uRxyCD3hBz'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's study patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up all night?\n",
    "\n",
    "Let's start by analysing President Trump's sleep patterns. We will create a count of the number of tweets by hour of the day. \n",
    "\n",
    "Here's how you can get the hour from a tweet timestamp using the `datetime` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'Twitter for iPhone', 'id_str': '815271067749060609', 'text': 'RT @realDonaldTrump: Happy Birthday @DonaldJTrumpJr!\\nhttps://t.co/uRxyCD3hBz', 'created_at': 'Sat Dec 31 18:59:04 +0000 2016', 'retweet_count': 9529, 'in_reply_to_user_id_str': None, 'favorite_count': 0, 'is_retweet': True}\n",
      "Sat Dec 31 18:59:04 +0000 2016\n",
      "2016 12 31 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get first tweet\n",
    "tw = tweet_data[0]\n",
    "print(tw)\n",
    "# We see the timestamp is at the field 'created_at'\n",
    "\n",
    "# Get timestamp of the tweet\n",
    "date_str = tw['created_at']\n",
    "print(date_str)\n",
    "\n",
    "# Make into datetime object, get the attributes of the result\n",
    "dt = datetime.strptime(date_str,'%a %b %d %H:%M:%S +0000 %Y') # specify format of time string\n",
    "print(dt.year, dt.month, dt.day, dt.hour)\n",
    "type(dt.hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now count all tweets by hour. One way to do this is creating a dictionary with keys as hours and values as counts.\n",
    "\n",
    "#### Sidebar: dictionary comprehension\n",
    "\n",
    "Python has a convenient way of reducing the work we need to do for writing loops called _comprehensions_. We can write a loop to create a dictionary in a single line as follows. The same kind of thing can be done to create lists too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 0,\n",
       " 2: 0,\n",
       " 3: 0,\n",
       " 4: 0,\n",
       " 5: 0,\n",
       " 6: 0,\n",
       " 7: 0,\n",
       " 8: 0,\n",
       " 9: 0,\n",
       " 10: 0,\n",
       " 11: 0,\n",
       " 12: 0,\n",
       " 13: 0,\n",
       " 14: 0,\n",
       " 15: 0,\n",
       " 16: 0,\n",
       " 17: 0,\n",
       " 18: 0,\n",
       " 19: 0,\n",
       " 20: 0,\n",
       " 21: 0,\n",
       " 22: 0,\n",
       " 23: 0}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize dictionary of zero hourly counts using dictionary comprehension\n",
    "# Dictionary specified as key->hour, value->zero for each hour value in the range\n",
    "hourly_counts = {hour:0 for hour in range(24)}\n",
    "hourly_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find this initialization useful in calculating the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here. \n",
    "\n",
    "# Initialize hourly_counts as above\n",
    "\n",
    "# loop through tweets in tweet_data\n",
    "# within loop: get timestamp of tweet as above\n",
    "# within loop: get hour as above\n",
    "# within loop: add one to dictionary value for relevant hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most common hour for tweeting? What can you say about the President's sleeping patterns? What additional analysis would you do?\n",
    "\n",
    "You can use `matplotlib` to plot the result. You can do a line chart following [the first example here](https://matplotlib.org/users/pyplot_tutorial.html), or a bar chart following [this example](https://pythonspot.com/en/matplotlib-bar-chart/).\n",
    "\n",
    "You could also check how this pattern changes over different months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who's tweeting?\n",
    "\n",
    "It appears that there are different sources for the tweets in the `source` field of the data. \n",
    "\n",
    "Create an hourly count of tweets by the different sources. Can you infer what this suggests about Trump's personal phone and the one his office uses for tweeting?\n",
    "\n",
    "Let's first find all the sources that are in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Twitter for Android', 'TweetDeck', 'Twitter for iPad', 'Mobile Web (M5)', 'Twitter Ads', 'Twitter for iPhone', 'Twitter Web Client', 'Periscope', 'Instagram', 'Media Studio'}\n"
     ]
    }
   ],
   "source": [
    "sources = set()\n",
    "for tweet in tweet_data:\n",
    "    if tweet['source'] not in sources:\n",
    "        sources.add(tweet['source'])   \n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are the most common sources and what do their timings suggest about usage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Initialize dictionary (or multiple) like above\n",
    "# Loop through tweets and add to counts like above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who's tweeting what?\n",
    "\n",
    "How do the contents of Mr Trump's tweets change depending on the source? We could do some really sophisticated analysis here through [sentiment analysis](http://text-processing.com/demo/sentiment/) of the tweet texts. For the purposes of this exercise, do the following calculations by tweet source:\n",
    "\n",
    "1. Find the fraction of tweets containing the word 'dumb' in either upper or lower case.\n",
    "\n",
    "2. Repeat for words you'd like, for example the ones suggested below.\n",
    "\n",
    "You can also repeat the analysis by source and hour, or look at different words or mentions of different Twitter users. \n",
    "\n",
    "Note you probably want to count both upper and lower case words together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "words = ['dumb', 'brexit', '#makeamericagreatagain', 'guns', 'dead', '#crookedhillary']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the web\n",
    "\n",
    "The Twitter API provides us with data conveniently in a JSON-structured format, but not all websites have as convenient ways for us to download data. If we still want to extract data from the site, we can access web pages directly using Python. \n",
    "\n",
    "When we access a web page directly, we'll replicate in Python what our web browser does when we visit a web page. We contact a website and load the web page, which is typically a HTML file. An HTML file is a text file that includes not only the content of a web page, but also instructions for the browser on how to display the content, for example in paragraphs, tables, bold font, links, and so on. \n",
    "\n",
    "When accessing the HTML text file in Python, we'll take advantage of this structure (e.g. identifiers for links) to extract the information we're interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the important problem of finding a good breakfast in the UK. [This article from the Guardian](https://www.theguardian.com/lifeandstyle/2017/jan/15/50-best-breakfasts-uk) purports to list the 50 best breakfasts in the UK.\n",
    "\n",
    "The list has interesting information, but in an unstructured form. If we wanted to remember these whenever we're visiting a different city, we'd probably want to store them in a file on our computer or online for easy access. \n",
    "\n",
    "We'll use Python to make this transformation.\n",
    "\n",
    "### HTML view\n",
    "\n",
    "The web page lists the restaurants, giving links, locations, and a short description of each. Let's look at how the undrlying HTML file looks like. When viewing a web page in a browser, we can typically do this with the shortcut `Ctrl + U`. This will open the HTML source in a new tab.\n",
    "\n",
    "The file is huge, with 3000 lines of text. We won't go into details of the HTML here, but instead look for the part in the file where the restaurants are presented. The first restaurant is in \"Lewannick\", so let's look for that in the source with `Ctrl + F`. We see the following result.\n",
    "\n",
    "```html\n",
    "<h2><strong><a href=\"http://www.coombesheadfarm.co.uk/\" data-link-name=\"in body link\" class=\"u-underline\">Coombeshead Farm</a></strong><strong>, Lewannick, Cornwall</strong><br></h2>\n",
    "<p>It might seem a bit bonkers to include an upmarket B&amp;B (from £90 a night) that doesn’t offer breakfast to non-residents (for the time being), but Coombeshead earns its place here because it does possibly the best breakfast in the UK, and certainly this writer’s favourite of 2016 anywhere.</p>\n",
    "```\n",
    "Compare this to the website as it is displayed in your browser and try to link the different parts together.\n",
    "\n",
    "\n",
    "Let's try to understand the first restaurant details from the HTML code. Above, the text \", Lewannick, Cornwall\" is packaged in between the HTML _tags_ `<strong>` and `</strong>`. The web browser uses HTML tags to display contents in a specific way, here in **bold** font. The tag `<strong>` is the opening tag which starts the emphasized text, and `</strong>` with a slash is the closing tag which ends the emphasis.\n",
    "\n",
    "\n",
    "Similarly, the text \"Coombeshead Farm\" is packaged in between the tags `<a>` and `</a>`:\n",
    "```html\n",
    "<a href=\"http://www.coombesheadfarm.co.uk/\" data-link-name=\"in body link\" class=\"u-underline\">Coombeshead Farm</a>\n",
    "```\n",
    "This is the HTML way of specifying a link to another page. The tag `<a href=\"http://www.coombesheadfarm.co.uk/\">` is the opening tag which creates the link to the restaurant's website. The tag `</a>` closes the tag: in between these, the text \"Coombeshead Farm\" is what gets displayed on the web page as the link text. Here, the tag has three _attributes_: `href` specifying the link address, and `data-link-name=\"in body link\"` and `class=\"u-underline\"`, which the browser uses to determine how the link is shown.\n",
    "\n",
    "There are many different HTML tags, which the web browser uses to display contents in a specific way. We will similarly use them to pick the data we'd like from the page.\n",
    "\n",
    "### Getting the data\n",
    "\n",
    "We'll use the `requests` library to download the page HTML into Python. The code below fetches the page, and stores the result in `r`, which is a \"response\" object that the library creates. The object has a `text` attribute, which contains the HTML of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!DOCTYPE html>\\n<html id=\"js-context\" class=\"js-off is-not-modern id--signed-out\" lang=\"en\" data-page-path=\"/lifeandstyle/2017/jan/15/50-best-breakfasts-uk\">\\n<head>\\n<meta charset=\"utf-8\"/>\\n<title>The 50 best breakfast places in the UK | Life and style | The Guardian</title>\\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"/>\\n<meta name=\"format-detection\" content=\"telephone=no\"/>\\n<meta name=\"HandheldFriendly\" content=\"True\"/>\\n<meta name=\"viewport\" content=\"width=device-width,minimum-scale=1,i'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.get(\"https://www.theguardian.com/lifeandstyle/2017/jan/15/50-best-breakfasts-uk\")\n",
    "r.text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll parse the HTML using the `BeautifulSoup 4` library. Let's import it and parse the text. The code below creates a `soup` object that the Beautiful Soup library works with. Similarly to a web browser, Beautiful Soup takes advantage of the tag structure of the web page, not to display it, but to parse for the information that we'd like to store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  \n",
    "soup = BeautifulSoup(r.text, 'html.parser')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting restaurants\n",
    "\n",
    "We'd like to get all 50 restaurants' names, locations, and websites. Let's take another look at the HTML source.\n",
    "\n",
    "```html\n",
    "<h2><strong><a href=\"http://www.coombesheadfarm.co.uk/\" data-link-name=\"in body link\" class=\"u-underline\">Coombeshead Farm</a></strong><strong>, Lewannick, Cornwall</strong><br></h2>\n",
    "<p>It might seem a bit bonkers to include an upmarket B&amp;B (from £90 a night) that doesn’t offer breakfast to non-residents (for the time being), but Coombeshead earns its place here because it does possibly the best breakfast in the UK, and certainly this writer’s favourite of 2016 anywhere.</p>\n",
    "```\n",
    "\n",
    "It looks like the structure of the data we'd like is roughly the following.\n",
    "```html\n",
    "<h2><strong><a href=\"WEBPAGE LINK\" data-link-name=\"in body link\" class=\"u-underline\">RESTAURANT NAME</a></strong><strong>, LOCATION</strong><br></h2>\n",
    "```\n",
    "There's an outer `<h2>` tag which specifies a header, a `<strong>` tag and an `<a>` tag fpr the link and the name, and another `<strong>` tag for the location.\n",
    "\n",
    "We'd like to pick the three capitalized parts for all restaurants. We can search for tags in Beautiful Soup as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h2><strong><a class=\"u-underline\" data-link-name=\"in body link\" href=\"http://www.coombesheadfarm.co.uk/\">Coombeshead Farm</a></strong><strong>, Lewannick, Cornwall</strong><br/></h2>, <h2><a class=\"u-underline\" data-link-name=\"in body link\" href=\"http://cafealfresco.co.uk/page/eat\"><strong>Café Alf Resco</strong></a><strong>, Dartmouth, Devon</strong></h2>]\n"
     ]
    }
   ],
   "source": [
    "results = soup.find_all('h2') # find all h2 tags\n",
    "print(results[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can extract the information we want from each restaurant. We can take the first result we found, and apply Beautiful Soup's find method to it again to find (the first) link. The link will have the _attribute_ text and contain the actual link as `link['href']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coombeshead Farm http://www.coombesheadfarm.co.uk/\n",
      "Lewannick, Cornwall\n"
     ]
    }
   ],
   "source": [
    "# Find link in first result\n",
    "link = results[0].find('a') # find link\n",
    "name = link.text # get text\n",
    "url = link['href'] # get link address\n",
    "print(name, url)\n",
    "\n",
    "# Find the location\n",
    "location = results[0].find_all('strong')[1].text\n",
    "location = location.lstrip(' ,') # get rid of leading comma and space\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping is messy\n",
    "\n",
    "Now try to write a loop that repeats this for all the restaurants. Store the results in a list containing dictionary entries as follows:\n",
    "```python\n",
    "[{'Restaurant':name, 'Web page':url, 'Location':location}]\n",
    "```\n",
    "\n",
    "At some point you will run into trouble. This is because we have not just picked all restaurants, but other things in the article that also have the `<h2>` tag. Some of the results will thus not contain the information we want, and we need to prune them  further. Try to work your loop around this, and other issues you'll run into, to pick the 50 restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "result_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your results, you can use the following code to write the dictionary into a csv file with the `csv` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# File column names\n",
    "csv_columns = ['Location','Restaurant','Web page']\n",
    "\n",
    "with open('best_breakfast_guardian.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "    writer.writeheader() # header row\n",
    "    for rest in result_list: # rest of rows\n",
    "        writer.writerow(rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we also wanted to store the description of each restaurant? One approach to do so is to use the hierarchical structure of the file's HTML tags with Beautiful Soup. We say that the tags at the same level in the hierarchy as our current tag are _siblings_, while those above (nesting the current tag) are _parents_ and those below (inside the current tag) are _children_.  \n",
    "\n",
    "Inspecting the HTML, we see that the tags between two `<h2>` tags are `<p>` tags for paragraphs. Starting from a `<h2>` tag, we can loop through the paragraphs with the `next_sibling` attribute until we run into the next `<h2>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<p>It might seem a bit bonkers to include an upmarket B&amp;B (from £90 a night) that doesn’t offer breakfast to non-residents (for the time being), but Coombeshead earns its place here because it does possibly the best breakfast in the UK, and certainly this writer’s favourite of 2016 anywhere.</p>\n",
      "\n",
      "\n",
      "<p>The love child of two celebrated chefs, <a class=\"u-underline\" data-link-name=\"in body link\" href=\"http://aprilbloomfield.com/\">April Bloomfield</a> (of The Spotted Pig and The Breslin in New York) and Tom Adams (of <a class=\"u-underline\" data-link-name=\"in body link\" href=\"http://www.pittcue.co.uk/\">Pittcue in London</a>), this idyllic Cornish farmhouse, set in 60-plus acres of picture-postcard meadow and woodland, serves up an equally idyllic breakfast. Adams and co go the extra mile over the most important meal of the day: everything is homegrown, made on site or sourced as locally as possible.</p>\n",
      "\n",
      "\n",
      "<p>From the granola, bircher muesli, honey and on-trend kombucha (for those who like that kind of thing) to the outrageously brilliant cooked offering – which comprises a thick slice of home-cured belly bacon, a pillow of scrambled eggs (most likely laid that morning by the chooks scratching away in the garden), proper sourdough toast (that is, packing a real crunch and smothered in home-churned butter – yes, really) and a homemade hog’s pudding that I still dream about – I can’t think of many better ways to start the day. Actually, scratch that: I can’t think of many places I’d rather be full stop.</p>\n",
      "\n",
      "\n",
      "<p>The dinners (Thursday to Sunday nights) are something else, too, and you don’t have to stay over to tuck into those. <br/><strong>What to order:</strong> Everything.</p>\n",
      "\n",
      "\n",
      "Here's the full text:\n",
      "\n",
      "It might seem a bit bonkers to include an upmarket B&B (from £90 a night) that doesn’t offer breakfast to non-residents (for the time being), but Coombeshead earns its place here because it does possibly the best breakfast in the UK, and certainly this writer’s favourite of 2016 anywhere. The love child of two celebrated chefs, April Bloomfield (of The Spotted Pig and The Breslin in New York) and Tom Adams (of Pittcue in London), this idyllic Cornish farmhouse, set in 60-plus acres of picture-postcard meadow and woodland, serves up an equally idyllic breakfast. Adams and co go the extra mile over the most important meal of the day: everything is homegrown, made on site or sourced as locally as possible. From the granola, bircher muesli, honey and on-trend kombucha (for those who like that kind of thing) to the outrageously brilliant cooked offering – which comprises a thick slice of home-cured belly bacon, a pillow of scrambled eggs (most likely laid that morning by the chooks scratching away in the garden), proper sourdough toast (that is, packing a real crunch and smothered in home-churned butter – yes, really) and a homemade hog’s pudding that I still dream about – I can’t think of many better ways to start the day. Actually, scratch that: I can’t think of many places I’d rather be full stop. The dinners (Thursday to Sunday nights) are something else, too, and you don’t have to stay over to tuck into those. What to order: Everything. \n"
     ]
    }
   ],
   "source": [
    "current = results[0]\n",
    "\n",
    "text = ''\n",
    "# Loop until find next restaurant, ie h2 tag\n",
    "while current.next_sibling.name != 'h2':\n",
    "    print(current.next_sibling) # Either <p> or a new line\n",
    "    current = current.next_sibling\n",
    "    if current.name is not None: # Only get the paragraph text\n",
    "        text += current.text + ' ' # Add a space between paragraphs\n",
    "print('Here\\'s the full text:\\n')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the web\n",
    "\n",
    "As you've seen with the example, web scraping can easily get messy. The less structured the data that we're trying to gather is, the more tricky it is to parse it. In practice, web scraping works best for \n",
    "* Structured pages. The page we've just scraped is a text article with some header structure. Scraping data from tables or other more structured forms is often much easier. \n",
    "* Static pages. Accessing dynamic content (eg JavaScript) is often impossible through the standard HTML approach we have used. Alternative ways exist, but require more work.\n",
    "\n",
    "Compared to downloading a file or using an API, web scraping is a fragile way to access a site. The HTML page could be changed, breaking your scraper, whereas APIs are often well maintained and robust. \n",
    "\n",
    "You'll have noticed that it would be very easy to crawl through a lot of pages on a site very quickly by looping requests. This could be on a news website, or for example looking for business analytics positions on an online jobs platform. Before you do this, note that many websites don't like crawlers: The Guardian's business model, for instance, depends on ad revenues, so they don't like their content being automatically accessed. Here we've downloaded just one page, but if you ran a big scraping operation on them, they might ban you (and your flatmates). They do have an [API](http://open-platform.theguardian.com/access/) for accessing content.\n",
    "\n",
    "In brief, scraping can be a powerful tool, but if there's an alternative way to get the data you need, don't scrape manually. If you do scrape data, be nice to the website and follow their terms of access.\n",
    "\n",
    "Here are some further resources on scraping:\n",
    "* [The Beautiful Soup documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) is very well written\n",
    "* [A success story with scraping](https://www.wired.com/2014/01/how-to-hack-okcupid/all/)\n",
    "* [A nice and well-documented application on the NY Times website](http://www.dataschool.io/python-web-scraping-of-president-trumps-lies/)\n",
    "* [Web data collection tasks from Stanford's Computational Journalism lab](https://github.com/stanfordjournalism/search-script-scrape), mostly through various US government APIs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
